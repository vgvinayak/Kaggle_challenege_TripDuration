---
title: "R Notebook"
output: md_document
        
---
#Kaggle link 
https://www.kaggle.com/c/nyc-taxi-trip-duration/data

#Aim
In this competition, Kaggle is challenging us to build a model that predicts the total ride duration of taxi trips in New York City. Our primary dataset is one released by the NYC Taxi and Limousine Commission, which includes pickup time, geo-coordinates, number of passengers, and several other variables.

# Reading and understanding data set

```{r}
library(plyr)
library(geosphere)
library(ggplot2)
library(ParamHelpers)
library(mlr)
library(xgboost)
library(caTools)
train<-read.csv("train.csv")
test<-read.csv("test.csv")

str(train)
summary(train)
```
We find:

-vendor_id only takes the values 1 or 2, presumably to differentiate two taxi companies

-pickup_datetime and (in the training set) dropoff_datetime are combinations of date and time that we will have to re-format into a more useful shape

-passenger_count takes a median of 1 and a maximum of 9 in both data sets

-The pickup/dropoff_longitute/latitute describes the geographical coordinates where the meter was activate/deactivated.

-store_and_fwd_flag is a flag that indicates whether the trip data was sent immediately to the vendor ("N") or held in the memory of the taxi because there was no connection to the server ("Y"). Maybe there could be a correlation with certain geographical areas with bad reception?

-trip_duration: our target feature in the training data is measured in seconds.


#Checking for NA values

```{r}
which(is.na(train)==T)
```
Hence there is no missing values

#Some pre-processing with training set
## Calculating distance and speed from Lattitude and Longitude

```{r}
train$distance<-apply(train,1,function(i) distCosine(c(as.numeric(i[6]),as.numeric(i[7])),c(as.numeric(i[8]),as.numeric(i[9]))))
train$distance<-train$distance/1000
train$speed<-3600*(train$distance/train$trip_duration)
```

-Creating distance variable ,here distance is in Kilometres
-Then calculating speed from that.

## Working with date and time

```{r}
train$pickup_datetime<-as.character(train$pickup_datetime)
pickupsplit_list<-strsplit(train$pickup_datetime,split = " ")
pickupsplit_df<-ldply(pickupsplit_list)
names(pickupsplit_df)<-c("pickup_date","pickup_time")
train$dropoff_datetime<-as.character(train$dropoff_datetime)
dropsplit_list<-strsplit(train$dropoff_datetime,split = " ")
dropsplit_df<-ldply(dropsplit_list)
names(dropsplit_df)<-c("drop_date","drop_time")
train<-cbind(train,pickupsplit_df)
train<-cbind(train,dropsplit_df)
head(train)
```

-We splitted the datatime variable to individual date and time variable
-Thus creating 4 variables , 2 for pickup and 2 for drop

## Converting date and time to the required format and adding revelant variable

```{r}
train$pickup_date<-as.Date(train$pickup_date,format = '%Y-%m-%d')
train$drop_date<-as.Date(train$drop_date,format = '%Y-%m-%d')
train$month<-months(train$pickup_date)
month_c<-as.data.frame(with(train, model.matrix(~ month + 0)))
train<-cbind(train,month_c)
train$weekdays<-weekdays(train$pickup_date)
weekdays_c<-as.data.frame(with(train,model.matrix(~weekdays + 0)))
train<-cbind(train,weekdays_c)
train$pickup_time<-strptime(train$pickup_time,"%H:%M:%S")
train$pickup_time<-as.POSIXct(train$pickup_time)
train$drop_time<-strptime(train$drop_time,"%H:%M:%S")
train$drop_time<-as.POSIXct(train$drop_time)
```

```{r}
colnames(train)
```

-Extracted Month and wekdays out of pick up date and then binary hotencoded to create variables starting with month and weekdays.

## Creating a "hour" variable to include the effect of time at whivh the trip started.

```{r}
train$hour<-substr(train$pickup_time, start = 12, stop = 13)
train$hour<-as.numeric(train$hour)
head(train)
```

-Divided the time interval in 1hour interval to create "hour" variable
-00:00:00 to 01:00:00 depicts 0 , 01:00:00 to 02:00:00 depicts 1 and so on.

#Some visualizations and exploratory analysis
```{r}
ggplot(data=train,aes(x=trip_duration))+geom_histogram(bins = 150)+scale_x_log10()+labs(x="Trip Duration",y="Count",title="Main distiribution of Time interval")
```

-Distribution of Timeinterval is smooth with around 1000 seconds as the mode
-There are many unrealistic rides with less then 10 seconds and around 10000 seconds.

## Thus treating these outliers
```{r}
mean(train[train$trip_duration<=60,]$speed)
quantile(train[train$trip_duration<=60,]$speed)
```

-These short trips looks real since average speed is considerable .

```{r}
mean(train[train$trip_duration>=72000,]$speed)
quantile(train[train$trip_duration>=72000,]$speed)
```

-These long trips are surely unrealistics since the average speed does not fall in normal range.
-Removing journey lasting greater then 20 hours

```{r}
train<-train[train$trip_duration<72000,]
```

```{r}
ggplot(data=train,aes(x=distance))+geom_histogram(bins = 200)+labs(x="Distance",y="Count",title="Main distiribution of Distance")
```
```{r}
summary(train$distance)
quantile(train$distance)
```

-There are some unrealistic journey with zero kilometre
-Also there are few with unrealistic high kilometres
-Removing These

```{r}
train<-train[train$distance!=0,]
train<-train[train$distance<500,]
```


```{r}
ggplot(data=train,aes(x=pickup_date))+geom_histogram(fill="red",bins = 150)+labs(x="Pickup_date",y="Count",title="Main distribution of dates")
```

-Sudden drop in the Pickups in Late january or early february.
-May be seasonor climate effecting the rides.

```{r}
ggplot(data=train,aes(x=as.factor(passenger_count),fill=as.factor(passenger_count)))+geom_bar(stat = "count")+labs(x="Passenger_count",y="Count",title="Main distribution of No. of passengers")
```

-On a general trend No. of rides decreases with increasing with no. of passenger from 1 to 4
-From 5 onwards we see increase in the ride due to the large cars may be.
-Majority riders are single passengers.

```{r}
ggplot(data=train,aes(x=as.factor(vendor_id),fill=as.factor(vendor_id)))+geom_bar(stat = "count")+labs(x="vendor_id",y="Count",title="Main distribution of Vendors")
```

-Vendor 2 has more trips then Vendor 1

```{r}
ggplot(data=train,aes(x=as.factor(store_and_fwd_flag),fill=as.factor(store_and_fwd_flag)))+geom_bar(stat = "count")+labs(x="store_and_fwd_flag",y="Count",title="Main distribution of store_and_fwd_flag")
```

-The store_and_fwd_flag values, indicating whether the trip data was sent immediately to the vendor ("N") or held in the memory of the taxi because there was no connection to the server ("Y"), show that there was almost no storing taking place

```{r}
positions<-c("Monday","Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday")
ggplot(data=train,aes(as.factor(weekdays),fill=as.factor(weekdays)))+geom_bar()+scale_x_discrete(limits = positions)+geom_line(stat = "count",aes(group=1)) +labs(x="Weedays",y="Count",title="Main distribution of weekdays")
```

-No.of trips increases from Monday to Friday and then decreases.
-Friday has the maximumNo. of trips
-Supprisingly Monday has the least trip despite being the working day.

```{r}
ggplot(data=train,aes(as.factor(hour)))+geom_point(stat = "count")+geom_line(stat = "count",aes(group=1))+labs(x="Hour",y="Count",title="Main distribution of hours")
```

-As esxpected No.of rides are lower between 00:00:00 and 6:00:00 , and they decreases as the time increases.
-No.of rides are higher after 6:00:00 depicting office hours.
-No. of rides are highest after 18:00:00 


```{r}
ggplot(data=train,aes(x=distance*1000,y=trip_duration))+geom_point()+labs(x="Distance in metres",y="trip_duration",title="Main distribution of Distance vs Time")+scale_x_log10() +
  scale_y_log10()
```

-On whole as expected time increases as distance of the tripincreases.We can see a linear trend in the data.

```{r}
positions<-c("Monday","Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday")
train$weekdays<-as.character(train$weekdays)
weekdaysframe<-data.frame(weekdays=character(),med=numeric())
train$trip_duration<-as.numeric(train$trip_duration)
for(i in positions){
  daysframe<-data.frame(weekdays=i,med=median(train[train$weekdays==i,]$trip_duration))
  weekdaysframe<-rbind(weekdaysframe,daysframe)
}
 ggplot(data=weekdaysframe,aes(x=weekdays,y=med))+geom_point()+labs(x="Weekdays",y="Median_trip_time",title="Main distribution of Trip_time vs Weekdays")+geom_line(aes(group=1))
```

-As expected median trip time increases from monday.
-Thrusday saw the maximum trip time.
-As expected triptime for Saturday and Sunday are lower

```{r}
train$month<-as.factor(train$month)
positions1<-levels(train$month)
monthframe<-data.frame(month=character(),med=numeric())
for(i in positions1){
  monthsframe<-data.frame(month=i,med=median(train[train$month==i,]$trip_duration))
  monthframe<-rbind(monthframe,monthsframe)
}
 ggplot(data=monthframe,aes(x=month,y=med))+geom_point()+labs(x="Months",y="Median_trip_time",title="Main distribution of Trip_time vs Months")
```

-January saw the lowest median trip time while June saw the highest.

```{r}
positions2<-c(0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23)
hourframe<-data.frame(hour=numeric(),med=numeric())
for(i in positions2){
  hoursframe<-data.frame(hour=i,med=median(train[train$hour==i,]$trip_duration))
  hourframe<-rbind(hourframe,hoursframe)
}
 ggplot(data=hourframe,aes(x=hour,y=med))+geom_point()+labs(x="Hour",y="Median_trip_time",title="Main distribution of Trip_time vs Hour")
```

-Trip between 14:00:00 and 15:00:00 saw the maxximum trip time while trip between 6:00:00 and 7:00:00 saw the least.
- points that are from 8:00:00 are high may be due to office hours

#Merging weather data with our training data by pickupdate
```{r}
weather<-read.csv("weather.csv")
names(weather)[1]<-c("pickup_date")
weather$pickup_date<-as.Date(weather$pickup_date,"%d-%m-%Y")
```

```{r}

train<-merge(train,weather,by="pickup_date")
```
```{r}
names(train)
```


#Preparing final traing set which include all required variable only

```{r}
filter<-c("pickup_date","pickup_datetime","dropoff_datetime","pickup_time","drop_date","drop_time","month","weekdays","speed")
finaltrain<-subset(train,select = !names(train)%in% filter)
```

# Hotencoding store_and_fwd_flag and converting all variables to numeric type

```{r}
finaltrain$store_and_fwd_flag<-as.character(finaltrain$store_and_fwd_flag)
finaltrain[finaltrain$store_and_fwd_flag=="N",]$store_and_fwd_flag<-0
finaltrain[finaltrain$store_and_fwd_flag=="Y",]$store_and_fwd_flag<-1
finaltrain<-sapply(finaltrain,as.numeric)
finaltrain<-as.data.frame(finaltrain)
```

# Checking for highly corelated independnt variables and thus removing them to create more robust model.

```{r}
cordata<-as.data.frame(cor(finaltrain[,-c(1)]))
apply(cordata,2,function(x)which(x>=0.8))


```

-maximum,minimum and average temperature are highly corelated.

```{r}
filter1<-c("minimum.temperature","average.temperature")
finaltrain<-subset(finaltrain,select = !names(finaltrain)%in% filter1)
```

-Removing minimum.temperature and average.temperature variable from model

#preparing test data to the required format

```{r}
test$distance<-apply(test,1,function(i) distCosine(c(as.numeric(i[5]),as.numeric(i[6])),c(as.numeric(i[7]),as.numeric(i[8]))))
test$distance<-test$distance/1000
test$pickup_datetime<-as.character(test$pickup_datetime)
pickupsplit_list<-strsplit(test$pickup_datetime,split = " ")
pickupsplit_df<-ldply(pickupsplit_list)
names(pickupsplit_df)<-c("pickup_date","pickup_time")
test<-cbind(test,pickupsplit_df)

```
```{r}
test$pickup_date<-as.Date(test$pickup_date,format = '%Y-%m-%d')
test$month<-months(test$pickup_date)
month_c<-as.data.frame(with(test, model.matrix(~ month + 0)))
test<-cbind(test,month_c)
test$weekdays<-weekdays(test$pickup_date)
weekdays_c<-as.data.frame(with(test,model.matrix(~weekdays + 0)))
test<-cbind(test,weekdays_c)
test$pickup_time<-strptime(test$pickup_time,"%H:%M:%S")
test$pickup_time<-as.POSIXct(test$pickup_time)
```
```{r}
test$hour<-substr(test$pickup_time, start = 12, stop = 13)
test$hour<-as.numeric(test$hour)
```
```{r}
test<-merge(test,weather,by="pickup_date")
```
```{r}
filter<-c("pickup_date","pickup_datetime","pickup_time","month","weekdays")
finaltest<-subset(test,select = !names(test)%in% filter)
```
```{r}
finaltest$store_and_fwd_flag<-as.character(finaltest$store_and_fwd_flag)
finaltest[finaltest$store_and_fwd_flag=="N",]$store_and_fwd_flag<-0
finaltest[finaltest$store_and_fwd_flag=="Y",]$store_and_fwd_flag<-1
id<-finaltest$id
finaltest<-sapply(finaltest,as.numeric)
finaltest<-as.data.frame(finaltest)
```
```{r}
filter1<-c("minimum.temperature","average.temperature")
finaltest<-subset(finaltest,select = !names(finaltest)%in% filter1)
```

# Training the XG-Boost model.

```{r}
duration<-(finaltrain$trip_duration)/60
xgb <- xgboost(data = data.matrix(finaltrain[,-c(1,9)]),
 label = duration,
 eta = 0.3,
 nround=70,
 subsample = 0.7,
 colsample_bytree = 0.7,
 seed = 1,
 eval_metric = "rmse",
 objective = "reg:linear",
 max_depth=8
)

```

# Cross-validating the model

```{r}
# xgb_cv <- xgb.cv(data=data.matrix(finaltrain[,-c(1,9)]),label = duration, 
#  eta = 0.3,
#  nround=100, 
#  subsample = 0.7,
#  colsample_bytree = 0.7,
#  seed = 1,
#  eval_metric = "rmse",
#  objective = "reg:linear",
#  nfold=5,
#  max_depth=8,
#  early.stop.round=3)
```


-Validating the modelby checking rmse and finding the optimum n round value
-Iteration stops at optimum value of nround

#Predicting the test data

```{r}
y_pred <- predict(xgb, data.matrix(finaltest[,-1]))
output<-data.frame(id=id,trip_duration_predicted=y_pred)
```

# Checking the importanceof features
```{r}
important <- as.data.frame(xgb.importance(feature_names = colnames(finaltrain[,-c(1,9)]), model = xgb))
ggplot(important,aes(x=Feature,y=Gain,fill=Feature))+geom_bar(stat = "identity")
```

-As expected Distance come out to be most important feature.


#A more accurate but time consuming way of traing the model (For supercomputers)

```{r}
finaltrain$trip_duration<-(finaltrain$trip_duration/60)
set.seed(1001)
  getParamSet("regr.xgboost")
  xg_set <- makeLearner("regr.xgboost", predict.type = "response")
  xg_set$par.vals <- list(
    objective = "reg:linear",
    eval_metric = "rmse",
    nrounds = 10
  )
  xg_ps <- makeParamSet(
    makeIntegerParam("nrounds",lower=9,upper=81),
    makeIntegerParam("max_depth",lower=6,upper=15),
    makeNumericParam("lambda",lower=0.50,upper=0.60),
    makeNumericParam("eta", lower = 0.001, upper = 0.3),
    makeNumericParam("subsample", lower = 0.50, upper = 1),
    makeNumericParam("min_child_weight",lower=1,upper=5),
    makeNumericParam("colsample_bytree",lower = 0.5,upper = 0.9)
  )
  rancontrol <- makeTuneControlRandom(maxit = 70L) #do 100 iterations
  set_cv <- makeResampleDesc("CV",iters = 3L)

    traintask_1 <- makeRegrTask(data=finaltrain[,-c(1)],target="trip_duration")
    xg_tune <- tuneParams(learner = xg_set, task = traintask_1, resampling = set_cv,measures =mse,par.set = xg_ps, control = rancontrol)
    #set parameters
    xg_new <- setHyperPars(learner = xg_set, par.vals = xg_tune$x)
    #train model
    xgmodel_acc <- train(xg_new, traintask_1)
    predict.xg <- predict(xgmodel_acc,newdata=finaltest[,-c(1)])
    pred<-predict.xg$data
    pred<-pred$response
    pred<-pred*60
    output<-data.frame(id=id,trip_duration_predicted=pred)
```

-This method provide a more detailed investigation of tunning the parameters by random search over a range of values of hyperparameters many times.
-It finally tune the parametrs to which the model results mmaximum validation set accuracy.